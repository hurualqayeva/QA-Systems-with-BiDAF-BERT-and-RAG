{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T22:25:39.914350Z",
     "start_time": "2025-05-07T22:25:30.178221Z"
    }
   },
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "BERT_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Max lengths for BERT token sequences (these will be the sequence lengths for BiDAF)\n",
    "MAX_BERT_QUESTION_LEN = 64  # Max BERT tokens for the question\n",
    "MAX_BERT_CONTEXT_LEN = 512 # Max BERT tokens for the context (where answer span is predicted)\n",
    "\n",
    "# Max characters per BERT token (for the character CNN)\n",
    "MAX_BERT_TOKEN_CHAR_LEN = 25 # Most BERT tokens (subwords) are short. Adjust if needed.\n",
    "\n",
    "# Paths for saving processed data and char vocab\n",
    "CHAR_VOCAB_BERT_PATH = \"./squad_char_vocab_for_bert_tokens.json\"\n",
    "PROCESSED_TRAIN_BERT_CHAR_PATH = \"./squad_train_processed_bert_char.hf\"\n",
    "PROCESSED_VAL_BERT_CHAR_PATH = \"./squad_val_processed_bert_char.hf\"\n",
    "\n",
    "# Special character tokens\n",
    "CHAR_PAD_TOKEN = \"<C_PAD>\"\n",
    "CHAR_UNK_TOKEN = \"<C_UNK>\" # For characters not in our char vocab (rare for BERT tokens)\n",
    "\n",
    "# Load BERT Tokenizer\n",
    "print(f\"Loading BERT tokenizer: {BERT_MODEL_NAME}...\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer: bert-base-cased...\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:26:00.700294Z",
     "start_time": "2025-05-07T22:25:52.558211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def build_char_vocab_from_bert_tokens(squad_dataset_split, bert_tokenizer, special_char_tokens):\n",
    "    print(\"Building character vocabulary from BERT tokens...\")\n",
    "    char_counter = Counter()\n",
    "    # Process a subset for speed, or all of train if needed\n",
    "    # For a robust char vocab, process all unique BERT tokens from tokenizer.vocab\n",
    "    # or tokenize a large representative text sample.\n",
    "\n",
    "    # Option 1: Characters from BERT's own vocabulary tokens\n",
    "    # for bert_token_str in bert_tokenizer.vocab.keys():\n",
    "    #     char_counter.update(bert_token_str)\n",
    "\n",
    "    # Option 2: Characters from tokenizing actual SQuAD data (more targeted)\n",
    "    # Let's use a sample from the provided SQuAD split for this example\n",
    "    sample_size = min(10000, len(squad_dataset_split)) # Adjust sample size\n",
    "    print(f\"Using a sample of {sample_size} examples to build character vocabulary...\")\n",
    "    for i in tqdm(range(sample_size)):\n",
    "        example = squad_dataset_split[i]\n",
    "        context_tokens = bert_tokenizer.tokenize(example['context'])\n",
    "        question_tokens = bert_tokenizer.tokenize(example['question'])\n",
    "        for bert_token_str in context_tokens + question_tokens:\n",
    "            char_counter.update(bert_token_str)\n",
    "\n",
    "    char_to_idx = {token: idx for idx, token in enumerate(special_char_tokens)}\n",
    "    idx_to_char = {idx: token for token, idx in char_to_idx.items()}\n",
    "\n",
    "    for char, count in char_counter.most_common():\n",
    "        if char not in char_to_idx: # Avoid re-adding special tokens if they are actual chars\n",
    "            idx = len(char_to_idx)\n",
    "            char_to_idx[char] = idx\n",
    "            idx_to_char[idx] = char\n",
    "\n",
    "    print(f\"Character vocabulary size (for BERT tokens): {len(char_to_idx)}\")\n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "# Load raw SQuAD data\n",
    "squad_raw: DatasetDict = load_dataset(\"squad\")\n",
    "squad_train_raw: Dataset = squad_raw['train']\n",
    "# squad_validation_raw: Dataset = squad_raw['validation'] # if needed for char vocab\n",
    "\n",
    "# Build and save character vocabulary\n",
    "# (Ideally run this once and save/load)\n",
    "if not os.path.exists(CHAR_VOCAB_BERT_PATH):\n",
    "    char_to_idx, idx_to_char = build_char_vocab_from_bert_tokens(squad_train_raw, tokenizer, [CHAR_PAD_TOKEN, CHAR_UNK_TOKEN])\n",
    "    print(f\"Saving character vocabulary to {CHAR_VOCAB_BERT_PATH}...\")\n",
    "    with open(CHAR_VOCAB_BERT_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'char_to_idx': char_to_idx, 'idx_to_char': idx_to_char}, f, ensure_ascii=False)\n",
    "    print(\"Character vocabulary saved.\")\n",
    "else:\n",
    "    print(f\"Loading existing character vocabulary from {CHAR_VOCAB_BERT_PATH}...\")\n",
    "    with open(CHAR_VOCAB_BERT_PATH, 'r', encoding='utf-8') as f:\n",
    "        char_vocab_data = json.load(f)\n",
    "        char_to_idx = char_vocab_data['char_to_idx']\n",
    "        # idx_to_char = char_vocab_data['idx_to_char'] # If needed\n",
    "print(f\"Character vocab size: {len(char_to_idx)}\")\n",
    "CHAR_PAD_ID = char_to_idx[CHAR_PAD_TOKEN]\n",
    "CHAR_UNK_ID = char_to_idx.get(CHAR_UNK_TOKEN, CHAR_PAD_ID) # Fallback for UNK if not explicitly made"
   ],
   "id": "d5efcdc9f438f489",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing character vocabulary from ./squad_char_vocab_for_bert_tokens.json...\n",
      "Character vocab size: 266\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:26:39.583230Z",
     "start_time": "2025-05-07T22:26:27.232499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_for_bert_bidaf_with_chars(examples):\n",
    "    questions = [q.strip() for q in examples['question']]\n",
    "    contexts = examples['context']\n",
    "    answers = examples['answers']\n",
    "    example_ids = examples['id']\n",
    "\n",
    "    # Tokenize questions with BERT\n",
    "    tokenized_questions = tokenizer(\n",
    "        questions,\n",
    "        max_length=MAX_BERT_QUESTION_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", # Pad to MAX_BERT_QUESTION_LEN\n",
    "        return_attention_mask=True,\n",
    "        return_offsets_mapping=False # Not needed for question if processed alone\n",
    "    )\n",
    "\n",
    "    # Tokenize contexts with BERT\n",
    "    tokenized_contexts = tokenizer(\n",
    "        contexts,\n",
    "        max_length=MAX_BERT_CONTEXT_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", # Pad to MAX_BERT_CONTEXT_LEN\n",
    "        return_attention_mask=True,\n",
    "        return_offsets_mapping=True # Essential for answer mapping\n",
    "    )\n",
    "\n",
    "    # Prepare lists for results\n",
    "    start_positions_bert_final = []\n",
    "    end_positions_bert_final = []\n",
    "    context_bert_token_char_ids_final = []\n",
    "    question_bert_token_char_ids_final = []\n",
    "\n",
    "    for i in range(len(contexts)):\n",
    "        # --- Answer Span Mapping ---\n",
    "        answer = answers[i]\n",
    "        context_char_to_token_offset = tokenized_contexts.offset_mapping[i]\n",
    "        # context_bert_ids_for_current_example = tokenized_contexts.input_ids[i] # For debugging\n",
    "\n",
    "        answer_char_start = answer['answer_start'][0]\n",
    "        answer_text = answer['text'][0]\n",
    "        answer_char_end = answer_char_start + len(answer_text)\n",
    "\n",
    "        # Find start BERT token index for the answer\n",
    "        start_token_idx_bert = -1\n",
    "        for token_idx, (start_char, end_char) in enumerate(context_char_to_token_offset):\n",
    "            if start_char <= answer_char_start < end_char:\n",
    "                start_token_idx_bert = token_idx\n",
    "                break\n",
    "\n",
    "        # Find end BERT token index for the answer\n",
    "        end_token_idx_bert = -1\n",
    "        if start_token_idx_bert != -1: # Only search for end if start was found\n",
    "            for token_idx in range(start_token_idx_bert, len(context_char_to_token_offset)):\n",
    "                start_char, end_char = context_char_to_token_offset[token_idx]\n",
    "                if start_char < answer_char_end <= end_char : # Answer ends within or at the end of this token\n",
    "                    end_token_idx_bert = token_idx\n",
    "                    break\n",
    "                if start_char >= answer_char_end : # Answer ended before this token (should have been caught)\n",
    "                    # This case means previous token was the end, or something is off.\n",
    "                    # For safety, if it passed the start, but now start_char is already beyond answer_char_end,\n",
    "                    # it implies the answer might be fully contained in start_token_idx_bert or previous tokens.\n",
    "                    # The above loop usually handles this better by checking answer_char_end <= end_char.\n",
    "                    # If answer text is empty, end_token_idx_bert might remain -1.\n",
    "                    if end_token_idx_bert == -1 and token_idx > start_token_idx_bert: # If not set and current token is past ans\n",
    "                         end_token_idx_bert = token_idx -1\n",
    "                    break\n",
    "            if end_token_idx_bert == -1 and start_token_idx_bert != -1: # If answer fully in start token or at end of context\n",
    "                 if answer_char_end <= context_char_to_token_offset[start_token_idx_bert][1]:\n",
    "                    end_token_idx_bert = start_token_idx_bert\n",
    "                 else: # Reaches end of context tokens, and answer also ends there or got truncated\n",
    "                    for token_idx in range(len(context_char_to_token_offset)-1, -1, -1):\n",
    "                        if context_char_to_token_offset[token_idx] != (0,0): # find last actual token\n",
    "                            end_token_idx_bert = token_idx\n",
    "                            break\n",
    "\n",
    "\n",
    "        # Validate and handle unmappable answers (SQuAD 1.1 should always have an answer)\n",
    "        # For BiDAF, logits are over MAX_BERT_CONTEXT_LEN\n",
    "        if start_token_idx_bert == -1 or end_token_idx_bert == -1 or \\\n",
    "           start_token_idx_bert >= MAX_BERT_CONTEXT_LEN or \\\n",
    "           end_token_idx_bert >= MAX_BERT_CONTEXT_LEN or \\\n",
    "           end_token_idx_bert < start_token_idx_bert:\n",
    "            # print(f\"WARN: Unmappable/Out-of-bounds answer for ID {example_ids[i]}. Char span ({answer_char_start}-{answer_char_end}), Text: '{answer_text}'. Defaulting to CLS/0.\")\n",
    "            # Map to CLS token (index 0) for now. Your loss function should ignore this if possible.\n",
    "            # Or use -1 if your CrossEntropyLoss ignore_index is -1.\n",
    "            start_positions_bert_final.append(0) # Assuming CLS is at index 0 and ignored or handled\n",
    "            end_positions_bert_final.append(0)\n",
    "        else:\n",
    "            start_positions_bert_final.append(start_token_idx_bert)\n",
    "            end_positions_bert_final.append(end_token_idx_bert)\n",
    "\n",
    "        # --- Character ID Generation for BERT Tokens ---\n",
    "        # For Context\n",
    "        current_context_bert_ids = tokenized_contexts.input_ids[i]\n",
    "        context_word_char_ids_list = []\n",
    "        for bert_token_id in current_context_bert_ids: # These are already padded to MAX_BERT_CONTEXT_LEN\n",
    "            if bert_token_id == tokenizer.pad_token_id: # If it's a padding BERT token\n",
    "                char_ids_for_token = [CHAR_PAD_ID] * MAX_BERT_TOKEN_CHAR_LEN\n",
    "            else:\n",
    "                bert_token_str = tokenizer.convert_ids_to_tokens(bert_token_id)\n",
    "                char_ids_for_token = [char_to_idx.get(char, CHAR_UNK_ID) for char in bert_token_str]\n",
    "                # Pad/truncate characters in this BERT token\n",
    "                if len(char_ids_for_token) > MAX_BERT_TOKEN_CHAR_LEN:\n",
    "                    char_ids_for_token = char_ids_for_token[:MAX_BERT_TOKEN_CHAR_LEN]\n",
    "                else:\n",
    "                    char_ids_for_token.extend([CHAR_PAD_ID] * (MAX_BERT_TOKEN_CHAR_LEN - len(char_ids_for_token)))\n",
    "            context_word_char_ids_list.append(char_ids_for_token)\n",
    "        context_bert_token_char_ids_final.append(context_word_char_ids_list)\n",
    "\n",
    "        # For Question\n",
    "        current_question_bert_ids = tokenized_questions.input_ids[i]\n",
    "        question_word_char_ids_list = []\n",
    "        for bert_token_id in current_question_bert_ids: # Padded to MAX_BERT_QUESTION_LEN\n",
    "            if bert_token_id == tokenizer.pad_token_id:\n",
    "                char_ids_for_token = [CHAR_PAD_ID] * MAX_BERT_TOKEN_CHAR_LEN\n",
    "            else:\n",
    "                bert_token_str = tokenizer.convert_ids_to_tokens(bert_token_id)\n",
    "                char_ids_for_token = [char_to_idx.get(char, CHAR_UNK_ID) for char in bert_token_str]\n",
    "                if len(char_ids_for_token) > MAX_BERT_TOKEN_CHAR_LEN:\n",
    "                    char_ids_for_token = char_ids_for_token[:MAX_BERT_TOKEN_CHAR_LEN]\n",
    "                else:\n",
    "                    char_ids_for_token.extend([CHAR_PAD_ID] * (MAX_BERT_TOKEN_CHAR_LEN - len(char_ids_for_token)))\n",
    "            question_word_char_ids_list.append(char_ids_for_token)\n",
    "        question_bert_token_char_ids_final.append(question_word_char_ids_list)\n",
    "\n",
    "    # Prepare final dictionary for the dataset's .map() function\n",
    "    processed_output = {\n",
    "        'context_input_ids': tokenized_contexts.input_ids,\n",
    "        'context_attention_mask': tokenized_contexts.attention_mask,\n",
    "        'context_token_char_ids': context_bert_token_char_ids_final, # (batch, MAX_BERT_CONTEXT_LEN, MAX_BERT_TOKEN_CHAR_LEN)\n",
    "\n",
    "        'question_input_ids': tokenized_questions.input_ids,\n",
    "        'question_attention_mask': tokenized_questions.attention_mask,\n",
    "        'question_token_char_ids': question_bert_token_char_ids_final, # (batch, MAX_BERT_QUESTION_LEN, MAX_BERT_TOKEN_CHAR_LEN)\n",
    "\n",
    "        'start_token_bert': start_positions_bert_final,\n",
    "        'end_token_bert': end_positions_bert_final,\n",
    "        'id': example_ids\n",
    "    }\n",
    "    return processed_output\n",
    "\n",
    "\n",
    "# --- Apply Preprocessing to SQuAD dataset ---\n",
    "print(\"\\nStarting SQuAD preprocessing for BERT with Character Embeddings...\")\n",
    "# squad_train_raw and squad_validation_raw should be your loaded Hugging Face squad datasets\n",
    "# Example:\n",
    "# squad_raw: DatasetDict = load_dataset(\"squad\")\n",
    "# squad_train_raw: Dataset = squad_raw['train']\n",
    "# squad_validation_raw: Dataset = squad_raw['validation']\n",
    "\n",
    "# Important: Remove original columns to avoid issues when saving or using the dataset later\n",
    "train_cols_to_remove = squad_train_raw.column_names\n",
    "val_cols_to_remove = squad_raw['validation'].column_names # Assuming 'validation' split exists\n",
    "\n",
    "processed_squad_train = squad_train_raw.map(\n",
    "    preprocess_for_bert_bidaf_with_chars,\n",
    "    batched=True, # Process in batches for efficiency\n",
    "    remove_columns=train_cols_to_remove # Remove old columns\n",
    ")\n",
    "processed_squad_val = squad_raw['validation'].map(\n",
    "    preprocess_for_bert_bidaf_with_chars,\n",
    "    batched=True,\n",
    "    remove_columns=val_cols_to_remove\n",
    ")\n",
    "\n",
    "print(\"\\nPreprocessing complete.\")\n",
    "print(\"Sample processed training example:\")\n",
    "print(processed_squad_train[0])\n",
    "\n",
    "# Save processed datasets\n",
    "print(f\"Saving processed training data to {PROCESSED_TRAIN_BERT_CHAR_PATH}...\")\n",
    "processed_squad_train.save_to_disk(PROCESSED_TRAIN_BERT_CHAR_PATH)\n",
    "print(f\"Saving processed validation data to {PROCESSED_VAL_BERT_CHAR_PATH}...\")\n",
    "processed_squad_val.save_to_disk(PROCESSED_VAL_BERT_CHAR_PATH)\n",
    "print(\"Processed datasets saved.\")"
   ],
   "id": "eac7f824f2ccdb39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SQuAD preprocessing for BERT with Character Embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4d9a8c18f33492b9b3a82066133f779"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 155\u001B[0m\n\u001B[0;32m    152\u001B[0m train_cols_to_remove \u001B[38;5;241m=\u001B[39m squad_train_raw\u001B[38;5;241m.\u001B[39mcolumn_names\n\u001B[0;32m    153\u001B[0m val_cols_to_remove \u001B[38;5;241m=\u001B[39m squad_raw[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcolumn_names \u001B[38;5;66;03m# Assuming 'validation' split exists\u001B[39;00m\n\u001B[1;32m--> 155\u001B[0m processed_squad_train \u001B[38;5;241m=\u001B[39m \u001B[43msquad_train_raw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreprocess_for_bert_bidaf_with_chars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Process in batches for efficiency\u001B[39;49;00m\n\u001B[0;32m    158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_cols_to_remove\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Remove old columns\u001B[39;49;00m\n\u001B[0;32m    159\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m processed_squad_val \u001B[38;5;241m=\u001B[39m squad_raw[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(\n\u001B[0;32m    161\u001B[0m     preprocess_for_bert_bidaf_with_chars,\n\u001B[0;32m    162\u001B[0m     batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    163\u001B[0m     remove_columns\u001B[38;5;241m=\u001B[39mval_cols_to_remove\n\u001B[0;32m    164\u001B[0m )\n\u001B[0;32m    166\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPreprocessing complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\ML\\Lib\\site-packages\\datasets\\arrow_dataset.py:560\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    553\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    555\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    556\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    558\u001B[0m }\n\u001B[0;32m    559\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 560\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    561\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    562\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\ML\\Lib\\site-packages\\datasets\\arrow_dataset.py:3073\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3067\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3068\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m hf_tqdm(\n\u001B[0;32m   3069\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3070\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3071\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3072\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3073\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrank\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_single\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdataset_kwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   3074\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   3075\u001B[0m \u001B[43m                \u001B[49m\u001B[43mshards_done\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n",
      "File \u001B[1;32m~\\ML\\Lib\\site-packages\\datasets\\arrow_dataset.py:3476\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3472\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   3473\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(shard\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[0;32m   3474\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[0;32m   3475\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3476\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3478\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_same_num_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3480\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3481\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3482\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[0;32m   3483\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[0;32m   3484\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3485\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\ML\\Lib\\site-packages\\datasets\\arrow_dataset.py:3338\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3336\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   3337\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 3338\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madditional_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfn_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3339\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[0;32m   3340\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3341\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[0;32m   3342\u001B[0m     }\n",
      "Cell \u001B[1;32mIn[3], line 107\u001B[0m, in \u001B[0;36mpreprocess_for_bert_bidaf_with_chars\u001B[1;34m(examples)\u001B[0m\n\u001B[0;32m    105\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    106\u001B[0m             char_ids_for_token\u001B[38;5;241m.\u001B[39mextend([CHAR_PAD_ID] \u001B[38;5;241m*\u001B[39m (MAX_BERT_TOKEN_CHAR_LEN \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(char_ids_for_token)))\n\u001B[1;32m--> 107\u001B[0m     context_word_char_ids_list\u001B[38;5;241m.\u001B[39mappend(char_ids_for_token)\n\u001B[0;32m    108\u001B[0m context_bert_token_char_ids_final\u001B[38;5;241m.\u001B[39mappend(context_word_char_ids_list)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;66;03m# For Question\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T22:27:05.467761Z",
     "start_time": "2025-05-07T22:27:05.451044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharEmbedding(nn.Module):\n",
    "    def __init__(self, char_vocab_size, char_embedding_dim, char_cnn_out_channels,\n",
    "                 char_cnn_kernel_size, char_padding_idx, dropout_rate):\n",
    "        super(CharEmbedding, self).__init__()\n",
    "\n",
    "        self.char_embedding = nn.Embedding(\n",
    "            num_embeddings=char_vocab_size,\n",
    "            embedding_dim=char_embedding_dim,\n",
    "            padding_idx=char_padding_idx\n",
    "        )\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=char_embedding_dim,\n",
    "            out_channels=char_cnn_out_channels,\n",
    "            kernel_size=char_cnn_kernel_size,\n",
    "            # Padding to maintain length for easier max-pooling across the sequence dimension later,\n",
    "            # or use specific padding based on kernel size.\n",
    "            # For kernel_size=5, padding=2 would keep length same if stride=1.\n",
    "        )\n",
    "        # The paper implies max-pooling over the resulting length of the convolution for each word.\n",
    "        # If conv output length is L_out = L_in - kernel_size + 1 + 2*padding.\n",
    "        # Let's use padding such that the output length is reasonable.\n",
    "        # A common approach: padding = (kernel_size - 1) // 2 for 'same' style padding.\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x_char_ids):\n",
    "        # x_char_ids: (batch_size, seq_len, max_word_len)\n",
    "        batch_size, seq_len, max_word_len = x_char_ids.shape\n",
    "\n",
    "        # Reshape for embedding: (batch_size * seq_len, max_word_len)\n",
    "        x_char_ids_reshaped = x_char_ids.view(-1, max_word_len)\n",
    "\n",
    "        # Embed characters: (batch_size * seq_len, max_word_len, char_embedding_dim)\n",
    "        char_emb = self.char_embedding(x_char_ids_reshaped)\n",
    "        char_emb = self.dropout(char_emb) # Apply dropout to character embeddings\n",
    "\n",
    "        # Permute for Conv1d: (batch_size * seq_len, char_embedding_dim, max_word_len)\n",
    "        char_emb_permuted = char_emb.permute(0, 2, 1)\n",
    "\n",
    "        # Convolution: (batch_size * seq_len, char_cnn_out_channels, convolved_len)\n",
    "        # The convolved_len depends on max_word_len, kernel_size, and padding.\n",
    "        # Example: if max_word_len=16, kernel=5, padding=0 -> convolved_len = 16-5+1 = 12\n",
    "        # Example: if max_word_len=16, kernel=5, padding=2 -> convolved_len = 16-5+1+2*2 = 16\n",
    "        char_conv_out = self.conv1d(char_emb_permuted)\n",
    "        char_conv_out = F.relu(char_conv_out) # Apply ReLU\n",
    "\n",
    "        # Max-pool over the convolved length dimension: (batch_size * seq_len, char_cnn_out_channels)\n",
    "        # The kernel_size for max_pool1d should be the full length of the convolved dimension.\n",
    "        char_pooled = F.max_pool1d(char_conv_out, kernel_size=char_conv_out.shape[2]).squeeze(2)\n",
    "\n",
    "        # Reshape back to (batch_size, seq_len, char_cnn_out_channels)\n",
    "        final_char_emb = char_pooled.view(batch_size, seq_len, -1)\n",
    "\n",
    "        return final_char_emb\n",
    "\n",
    "\n",
    "# HighwayNetwork class remains the same as provided in the previous full model response\n",
    "class HighwayNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_layers):\n",
    "        super(HighwayNetwork, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.transform_gates = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        self.normal_layers = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            transform_gate_output = torch.sigmoid(self.transform_gates[i](x))\n",
    "            normal_layer_output = F.relu(self.normal_layers[i](x))\n",
    "            x = transform_gate_output * normal_layer_output + (1 - transform_gate_output) * x\n",
    "        return x\n",
    "\n",
    "# BiDAFAttention class remains the same as provided in the previous full model response\n",
    "class BiDAFAttention(nn.Module):\n",
    "    def __init__(self, hidden_size_times_2d): # effectively 2 * d, e.g., 2 * 300 = 600\n",
    "        super(BiDAFAttention, self).__init__()\n",
    "        self.hidden_size_times_2d = hidden_size_times_2d\n",
    "        self.similarity_weight = nn.Linear(self.hidden_size_times_2d * 3, 1, bias=False)\n",
    "\n",
    "    def forward(self, C_contextual, Q_contextual, C_mask, Q_mask):\n",
    "        # C_contextual: (batch, C_len, 2d)\n",
    "        # Q_contextual: (batch, Q_len, 2d)\n",
    "        batch_size, C_len, _ = C_contextual.shape\n",
    "        _, Q_len, _ = Q_contextual.shape\n",
    "\n",
    "        C_expanded = C_contextual.unsqueeze(2).expand(-1, -1, Q_len, -1)\n",
    "        Q_expanded = Q_contextual.unsqueeze(1).expand(-1, C_len, -1, -1)\n",
    "        elementwise_prod = C_expanded * Q_expanded\n",
    "        concat_features = torch.cat((C_expanded, Q_expanded, elementwise_prod), dim=3)\n",
    "        S = self.similarity_weight(concat_features).squeeze(3)\n",
    "\n",
    "        S_masked_q = S.masked_fill(Q_mask.unsqueeze(1) == 0, -float('inf'))\n",
    "        S_masked_c = S.masked_fill(C_mask.unsqueeze(2) == 0, -float('inf'))\n",
    "\n",
    "        alpha = F.softmax(S_masked_q, dim=2)\n",
    "        A = torch.bmm(alpha, Q_contextual)\n",
    "\n",
    "        max_S_c = torch.max(S, dim=2)[0]\n",
    "        max_S_c_masked = max_S_c.masked_fill(C_mask == 0, -float('inf'))\n",
    "        b_weights = F.softmax(max_S_c_masked, dim=1)\n",
    "        C_prime = torch.bmm(b_weights.unsqueeze(1), C_contextual).squeeze(1)\n",
    "        B = C_prime.unsqueeze(1).expand(-1, C_len, -1)\n",
    "\n",
    "        g_c_a = C_contextual * A\n",
    "        g_c_b = C_contextual * B\n",
    "        G = torch.cat((C_contextual, A, g_c_a, g_c_b), dim=2)\n",
    "        return G\n",
    "\n",
    "# Assume CharEmbedding, HighwayNetwork, BiDAFAttention classes are defined as before.\n",
    "# CharEmbedding parameters will be:\n",
    "CHAR_EMB_DIM_FOR_BERT_TOKENS = 8 # As per original BiDAF paper's char embedding dim\n",
    "CHAR_CNN_OUT_CHANNELS_FOR_BERT_TOKENS = 50 # Output dim of char CNN\n",
    "CHAR_CNN_KERNEL_SIZE_FOR_BERT_TOKENS = 5\n",
    "\n",
    "class BiDAF_BERT_Char(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert_model_name,\n",
    "                 # Char embedding params (for chars of BERT tokens)\n",
    "                 char_vocab_size, # Size of your new char_to_idx for BERT token chars\n",
    "                 char_embedding_dim, # e.g., 8\n",
    "                 char_cnn_out_channels, # e.g., 100\n",
    "                 char_cnn_kernel_size, # e.g., 5\n",
    "                 char_padding_idx,\n",
    "                 # BiDAF specific params\n",
    "                 hidden_size, # 'd' for BiDAF's LSTMs (e.g., 128 or 300)\n",
    "                 num_highway_layers=2,\n",
    "                 dropout_rate=0.2,\n",
    "                 num_modeling_lstm_layers=2\n",
    "                ):\n",
    "        super(BiDAF_BERT_Char, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False # Freeze BERT parameters\n",
    "        self.bert_hidden_dim = self.bert.config.hidden_size # 768 for bert-base\n",
    "\n",
    "        self.char_embedding_layer = CharEmbedding(\n",
    "            char_vocab_size=char_vocab_size,\n",
    "            char_embedding_dim=char_embedding_dim,\n",
    "            char_cnn_out_channels=char_cnn_out_channels,\n",
    "            char_cnn_kernel_size=char_cnn_kernel_size,\n",
    "            char_padding_idx=char_padding_idx,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.combined_input_dim = self.bert_hidden_dim + char_cnn_out_channels # e.g., 768 + 100 = 868\n",
    "\n",
    "        self.hidden_size = hidden_size # 'd' for BiDAF LSTMs\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.highway_network = HighwayNetwork(input_dim=self.combined_input_dim, num_layers=num_highway_layers)\n",
    "\n",
    "        self.contextual_lstm = nn.LSTM(\n",
    "            input_size=self.combined_input_dim,\n",
    "            hidden_size=self.hidden_size, # d\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0\n",
    "        )\n",
    "\n",
    "        self.attention = BiDAFAttention(hidden_size_times_2d=2 * self.hidden_size)\n",
    "\n",
    "        self.modeling_lstm = nn.LSTM(\n",
    "            input_size=8 * self.hidden_size,\n",
    "            hidden_size=self.hidden_size, # d\n",
    "            num_layers=num_modeling_lstm_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_rate if num_modeling_lstm_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        self.start_output_linear = nn.Linear(10 * self.hidden_size, 1)\n",
    "        self.end_output_linear = nn.Linear(10 * self.hidden_size, 1)\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self,\n",
    "                context_bert_ids, context_bert_mask, context_bert_char_ids,\n",
    "                question_bert_ids, question_bert_mask, question_bert_char_ids):\n",
    "\n",
    "        # 1. Get BERT Embeddings\n",
    "        C_bert_emb = self.bert(input_ids=context_bert_ids, attention_mask=context_bert_mask).last_hidden_state\n",
    "        Q_bert_emb = self.bert(input_ids=question_bert_ids, attention_mask=question_bert_mask).last_hidden_state\n",
    "\n",
    "        # 2. Get Character Embeddings for BERT tokens\n",
    "        C_char_level_emb = self.char_embedding_layer(context_bert_char_ids)\n",
    "        Q_char_level_emb = self.char_embedding_layer(question_bert_char_ids)\n",
    "\n",
    "        # 3. Concatenate BERT embeddings and Character-level embeddings for BERT tokens\n",
    "        C_combined_emb = torch.cat((C_bert_emb, C_char_level_emb), dim=2)\n",
    "        Q_combined_emb = torch.cat((Q_bert_emb, Q_char_level_emb), dim=2)\n",
    "\n",
    "        C_combined_emb = self.dropout_layer(C_combined_emb)\n",
    "        Q_combined_emb = self.dropout_layer(Q_combined_emb)\n",
    "\n",
    "        # 4. Highway Network\n",
    "        C_highway = self.highway_network(C_combined_emb)\n",
    "        Q_highway = self.highway_network(Q_combined_emb)\n",
    "\n",
    "        # 5. Contextual Embedding Layer (BiDAF's BiLSTM)\n",
    "        C_contextual, _ = self.contextual_lstm(C_highway)\n",
    "        Q_contextual, _ = self.contextual_lstm(Q_highway)\n",
    "\n",
    "        C_contextual = self.dropout_layer(C_contextual)\n",
    "        Q_contextual = self.dropout_layer(Q_contextual)\n",
    "\n",
    "        # BiDAF masks (from BERT attention masks)\n",
    "        C_bidaf_mask = context_bert_mask.float()\n",
    "        Q_bidaf_mask = question_bert_mask.float()\n",
    "\n",
    "        # 6. Attention Flow Layer\n",
    "        G = self.attention(C_contextual, Q_contextual, C_bidaf_mask, Q_bidaf_mask)\n",
    "        G = self.dropout_layer(G)\n",
    "\n",
    "        # 7. Modeling Layer\n",
    "        M, _ = self.modeling_lstm(G)\n",
    "        M = self.dropout_layer(M)\n",
    "\n",
    "        # 8. Output Layer\n",
    "        output_features = torch.cat((G, M), dim=2)\n",
    "        start_logits = self.start_output_linear(output_features).squeeze(2)\n",
    "        end_logits = self.end_output_linear(output_features).squeeze(2)\n",
    "\n",
    "        start_logits_masked = start_logits.masked_fill(C_bidaf_mask == 0, -float('inf'))\n",
    "        end_logits_masked = end_logits.masked_fill(C_bidaf_mask == 0, -float('inf'))\n",
    "\n",
    "        return start_logits_masked, end_logits_masked"
   ],
   "id": "d801baff5b0f22a2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T12:53:40.817938Z",
     "start_time": "2025-05-07T22:27:06.081708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk # To load Hugging Face datasets\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm # For progress bars\n",
    "from transformers import BertModel, BertTokenizerFast # Needed for model definition and tokenizer info\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "# Paths\n",
    "PROCESSED_TRAIN_PATH = \"./squad_train_processed_bert_char.hf\" # From previous preprocessing\n",
    "PROCESSED_VAL_PATH = \"./squad_val_processed_bert_char.hf\"   # From previous preprocessing\n",
    "CHAR_VOCAB_PATH = \"./squad_char_vocab_for_bert_tokens.json\" # From previous preprocessing\n",
    "MODEL_SAVE_DIR = \"./saved_models\"\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, \"bidaf_bert_char_best.pt\")\n",
    "\n",
    "# Ensure save directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Model & Embedding Hyperparameters\n",
    "BERT_MODEL_NAME = 'bert-base-cased' # Should match preprocessing\n",
    "# Character Embeddings (for chars of BERT tokens)\n",
    "CHAR_EMBEDDING_DIM = 25   # Dimension of individual char embedding (e.g. 8, 16, 20)\n",
    "CHAR_CNN_OUT_CHANNELS = 50 # Output dim of char CNN\n",
    "CHAR_CNN_KERNEL_SIZE = 5\n",
    "# BiDAF Structure\n",
    "# HIDDEN_SIZE = 300 # 'd' in BiDAF paper, for BiDAF's LSTMs\n",
    "HIDDEN_SIZE = 128 # User indicated a smaller model worked well\n",
    "NUM_HIGHWAY_LAYERS = 2\n",
    "NUM_MODELING_LSTM_LAYERS = 2 # User indicated a smaller model worked well\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# Training Hyperparameters\n",
    "LEARNING_RATE = 1e-3 # Often smaller for BERT-based models\n",
    "BATCH_SIZE = 16      # Adjust based on GPU memory (BERT is memory intensive)\n",
    "NUM_EPOCHS = 10       # Start with a few for BERT fine-tuning\n",
    "CLIP_GRAD_NORM = 5.0 # Common for BERT fine-tuning\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Memory Allocated: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB\")\n",
    "    print(f\"CUDA Memory Cached: {torch.cuda.memory_reserved(0)/1024**2:.2f} MB\")\n",
    "\n",
    "\n",
    "# --- Load Character Vocabulary ---\n",
    "try:\n",
    "    with open(CHAR_VOCAB_PATH, 'r', encoding='utf-8') as f:\n",
    "        char_vocab_data = json.load(f)\n",
    "        char_to_idx = char_vocab_data['char_to_idx']\n",
    "    CHAR_VOCAB_SIZE = len(char_to_idx)\n",
    "    CHAR_PADDING_IDX = char_to_idx.get(\"<C_PAD>\", 0) # Ensure your CHAR_PAD token name\n",
    "    print(f\"Character vocabulary loaded. Size: {CHAR_VOCAB_SIZE}, CHAR_PADDING_IDX: {CHAR_PADDING_IDX}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Character vocabulary file not found at {CHAR_VOCAB_PATH}. {e}\")\n",
    "    exit()\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Special character token {e} not found in loaded char vocabulary.\")\n",
    "    exit()\n",
    "\n",
    "# --- Load Processed Datasets ---\n",
    "try:\n",
    "    print(f\"Loading processed training data from {PROCESSED_TRAIN_PATH}...\")\n",
    "    train_dataset_processed = load_from_disk(PROCESSED_TRAIN_PATH)\n",
    "    print(f\"Loading processed validation data from {PROCESSED_VAL_PATH}...\")\n",
    "    val_dataset_processed = load_from_disk(PROCESSED_VAL_PATH)\n",
    "    print(\"Processed datasets loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading processed datasets: {e}\")\n",
    "    print(\"Please ensure your processed data paths are correct and data exists from the previous step.\")\n",
    "    exit()\n",
    "\n",
    "# Set format for PyTorch\n",
    "# These columns were created by `preprocess_for_bert_bidaf_with_chars`\n",
    "columns_to_torch = [\n",
    "    'context_input_ids', 'context_attention_mask', 'context_token_char_ids',\n",
    "    'question_input_ids', 'question_attention_mask', 'question_token_char_ids',\n",
    "    'start_token_bert', 'end_token_bert'\n",
    "]\n",
    "train_dataset_processed.set_format(type='torch', columns=columns_to_torch)\n",
    "val_dataset_processed.set_format(type='torch', columns=columns_to_torch)\n",
    "\n",
    "# --- DataLoaders ---\n",
    "train_dataloader = DataLoader(train_dataset_processed, batch_size=BATCH_SIZE, shuffle=True, num_workers=2 if device.type == 'cuda' else 0)\n",
    "val_dataloader = DataLoader(val_dataset_processed, batch_size=BATCH_SIZE, shuffle=False, num_workers=2 if device.type == 'cuda' else 0)\n",
    "print(f\"DataLoaders created. Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}\")\n",
    "\n",
    "# --- Model Instantiation ---\n",
    "model = BiDAF_BERT_Char(\n",
    "    bert_model_name=BERT_MODEL_NAME,\n",
    "    char_vocab_size=CHAR_VOCAB_SIZE,\n",
    "    char_embedding_dim=CHAR_EMBEDDING_DIM,\n",
    "    char_cnn_out_channels=CHAR_CNN_OUT_CHANNELS,\n",
    "    char_cnn_kernel_size=CHAR_CNN_KERNEL_SIZE,\n",
    "    char_padding_idx=CHAR_PADDING_IDX,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_highway_layers=NUM_HIGHWAY_LAYERS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    num_modeling_lstm_layers=NUM_MODELING_LSTM_LAYERS\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"BiDAF_BERT_Char Model instantiated. Trainable Parameters: {total_params:,}\")\n",
    "\n",
    "# --- Loss Function and Optimizer ---\n",
    "# IMPORTANT: Regarding ignore_index for CrossEntropyLoss:\n",
    "# The preprocessing step `preprocess_for_bert_bidaf_with_chars` mapped unmappable/out-of-bounds\n",
    "# answers to index 0 (CLS token index). If 0 can also be a valid start/end token for an answer,\n",
    "# then ignore_index=0 will incorrectly ignore those valid answers.\n",
    "# A safer approach is to ensure preprocessing maps such cases to a dedicated value like -100\n",
    "# and set ignore_index=-100. For now, assuming 0 is used for unmappable and it's okay.\n",
    "# If many answers are at token 0, this might need revisiting.\n",
    "# If your preprocessing was updated to use -1 for unmappable spans, use ignore_index=-1.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # Or -1 if your preprocessing maps unmappable to -1.\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE) # AdamW is often preferred for BERT\n",
    "\n",
    "# --- Metrics Calculation (Simplified Token-Level) ---\n",
    "def compute_metrics(pred_starts, pred_ends, true_starts, true_ends):\n",
    "    em_sum = 0\n",
    "    f1_sum = 0\n",
    "    num_examples = len(pred_starts)\n",
    "    if num_examples == 0: return 0.0, 0.0\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        ps, pe = pred_starts[i], pred_ends[i]\n",
    "        ts, te = true_starts[i], true_ends[i]\n",
    "\n",
    "        # Handle potential ignore_index values in true labels if they weren't filtered\n",
    "        if ts == criterion.ignore_index or te == criterion.ignore_index:\n",
    "            num_examples -=1 # Don't count this for EM/F1 if it was an ignored sample\n",
    "            continue\n",
    "\n",
    "        # Ensure predicted end is not before predicted start\n",
    "        if pe < ps: pe = ps\n",
    "\n",
    "        # Exact Match (token level)\n",
    "        if ps == ts and pe == te:\n",
    "            em_sum += 1\n",
    "\n",
    "        # F1 Score (token level)\n",
    "        pred_tokens = set(range(ps, pe + 1))\n",
    "        true_tokens = set(range(ts, te + 1))\n",
    "\n",
    "        common_tokens = len(pred_tokens.intersection(true_tokens))\n",
    "        if common_tokens == 0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            precision = common_tokens / len(pred_tokens) if len(pred_tokens) > 0 else 0\n",
    "            recall = common_tokens / len(true_tokens) if len(true_tokens) > 0 else 0\n",
    "            f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_sum += f1\n",
    "\n",
    "    return (em_sum / num_examples) * 100 if num_examples > 0 else 0.0, \\\n",
    "           (f1_sum / num_examples) * 100 if num_examples > 0 else 0.0\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, clip_norm):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False, dynamic_ncols=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        try:\n",
    "            context_ids = batch['context_input_ids'].to(device)\n",
    "            context_mask = batch['context_attention_mask'].to(device)\n",
    "            context_char_ids = batch['context_token_char_ids'].to(device)\n",
    "            question_ids = batch['question_input_ids'].to(device)\n",
    "            question_mask = batch['question_attention_mask'].to(device)\n",
    "            question_char_ids = batch['question_token_char_ids'].to(device)\n",
    "            true_start_indices = batch['start_token_bert'].to(device)\n",
    "            true_end_indices = batch['end_token_bert'].to(device)\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError in batch: {e}. Available keys: {batch.keys()}\")\n",
    "            raise e\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_logits, end_logits = model(\n",
    "            context_bert_ids=context_ids, context_bert_mask=context_mask, context_bert_char_ids=context_char_ids,\n",
    "            question_bert_ids=question_ids, question_bert_mask=question_mask, question_bert_char_ids=question_char_ids\n",
    "        )\n",
    "\n",
    "        # Filter out ignored indices before loss calculation if they are not handled by ignore_index\n",
    "        # This is important if ignore_index is, e.g., 0, and 0 can be a valid target.\n",
    "        # However, CrossEntropyLoss with ignore_index should handle this.\n",
    "        # Let's assume ignore_index in criterion handles it.\n",
    "\n",
    "        loss_start = criterion(start_logits, true_start_indices)\n",
    "        loss_end = criterion(end_logits, true_end_indices)\n",
    "        total_loss = loss_start + loss_end\n",
    "\n",
    "        # Handle cases where loss might be NaN (e.g., if all targets in a batch are ignored)\n",
    "        if torch.isnan(total_loss):\n",
    "            print(\"Warning: NaN loss detected. Skipping batch.\")\n",
    "            # Optionally, print details of the batch that caused NaN\n",
    "            # print(\"Problematic batch start_logits:\", start_logits)\n",
    "            # print(\"Problematic batch true_start_indices:\", true_start_indices)\n",
    "            continue # Skip optimizer step and loss accumulation for this batch\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        progress_bar.set_postfix({'loss': f'{total_loss.item():.4f}'})\n",
    "\n",
    "    return epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
    "\n",
    "# --- Evaluation Loop ---\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_pred_starts, all_pred_ends = [], []\n",
    "    all_true_starts, all_true_ends = [], []\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False, dynamic_ncols=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            context_ids = batch['context_input_ids'].to(device)\n",
    "            context_mask = batch['context_attention_mask'].to(device)\n",
    "            context_char_ids = batch['context_token_char_ids'].to(device)\n",
    "            question_ids = batch['question_input_ids'].to(device)\n",
    "            question_mask = batch['question_attention_mask'].to(device)\n",
    "            question_char_ids = batch['question_token_char_ids'].to(device)\n",
    "            true_start_indices = batch['start_token_bert'].to(device)\n",
    "            true_end_indices = batch['end_token_bert'].to(device)\n",
    "\n",
    "            start_logits, end_logits = model(\n",
    "                context_bert_ids=context_ids, context_bert_mask=context_mask, context_bert_char_ids=context_char_ids,\n",
    "                question_bert_ids=question_ids, question_bert_mask=question_mask, question_bert_char_ids=question_char_ids\n",
    "            )\n",
    "\n",
    "            loss_start = criterion(start_logits, true_start_indices)\n",
    "            loss_end = criterion(end_logits, true_end_indices)\n",
    "            total_loss = loss_start + loss_end\n",
    "\n",
    "            if not torch.isnan(total_loss): # Only accumulate if loss is valid\n",
    "                 epoch_loss += total_loss.item()\n",
    "\n",
    "            pred_start_batch = torch.argmax(start_logits, dim=1)\n",
    "            pred_end_batch = torch.argmax(end_logits, dim=1)\n",
    "\n",
    "            all_pred_starts.extend(pred_start_batch.cpu().tolist())\n",
    "            all_pred_ends.extend(pred_end_batch.cpu().tolist())\n",
    "            all_true_starts.extend(true_start_indices.cpu().tolist())\n",
    "            all_true_ends.extend(true_end_indices.cpu().tolist())\n",
    "\n",
    "            progress_bar.set_postfix({'loss': f'{total_loss.item() if not torch.isnan(total_loss) else \"NaN\":.4f}'})\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
    "    em, f1 = compute_metrics(all_pred_starts, all_pred_ends, all_true_starts, all_true_ends)\n",
    "\n",
    "    return avg_loss, em, f1\n",
    "\n",
    "# --- Main Training Orchestration ---\n",
    "best_val_f1 = -1.0\n",
    "\n",
    "print(\"\\nStarting training with BiDAF_BERT_Char model...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device, CLIP_GRAD_NORM)\n",
    "    val_loss, val_em, val_f1 = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Summary:\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"\\tVal Loss  : {val_loss:.4f} | Val EM: {val_em:.2f}% | Val F1: {val_f1:.2f}%\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"\\tNew best model saved to {BEST_MODEL_PATH} (F1: {best_val_f1:.2f}%)\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    print(f\"Best Validation F1: {best_val_f1:.2f}% (Model saved at {BEST_MODEL_PATH})\")\n",
    "else:\n",
    "    print(f\"No model was saved. Best Validation F1: {best_val_f1:.2f}%\")\n"
   ],
   "id": "d0032baa89208fca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA Device Name: NVIDIA GeForce GTX 1080\n",
      "CUDA Memory Allocated: 0.00 MB\n",
      "CUDA Memory Cached: 0.00 MB\n",
      "Character vocabulary loaded. Size: 266, CHAR_PADDING_IDX: 0\n",
      "Loading processed training data from ./squad_train_processed_bert_char.hf...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "022247477755463ea11befc7250a7cd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed validation data from ./squad_val_processed_bert_char.hf...\n",
      "Processed datasets loaded.\n",
      "DataLoaders created. Train batches: 5475, Val batches: 661\n",
      "BiDAF_BERT_Char Model instantiated. Trainable Parameters: 5,243,760\n",
      "\n",
      "Starting training with BiDAF_BERT_Char model...\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Summary:\n",
      "\tTrain Loss: 5.1313\n",
      "\tVal Loss  : 3.2444 | Val EM: 42.05% | Val F1: 59.12%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 59.12%)\n",
      "\n",
      "--- Epoch 2/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Summary:\n",
      "\tTrain Loss: 3.3016\n",
      "\tVal Loss  : 2.6746 | Val EM: 49.77% | Val F1: 67.66%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 67.66%)\n",
      "\n",
      "--- Epoch 3/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Summary:\n",
      "\tTrain Loss: 2.9623\n",
      "\tVal Loss  : 2.5492 | Val EM: 51.78% | Val F1: 69.41%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 69.41%)\n",
      "\n",
      "--- Epoch 4/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Summary:\n",
      "\tTrain Loss: 2.8030\n",
      "\tVal Loss  : 2.4688 | Val EM: 52.21% | Val F1: 70.37%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 70.37%)\n",
      "\n",
      "--- Epoch 5/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Summary:\n",
      "\tTrain Loss: 2.6992\n",
      "\tVal Loss  : 2.4195 | Val EM: 52.84% | Val F1: 70.92%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 70.92%)\n",
      "\n",
      "--- Epoch 6/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Summary:\n",
      "\tTrain Loss: 2.6292\n",
      "\tVal Loss  : 2.4440 | Val EM: 52.90% | Val F1: 71.32%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 71.32%)\n",
      "\n",
      "--- Epoch 7/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Summary:\n",
      "\tTrain Loss: 2.5791\n",
      "\tVal Loss  : 2.3963 | Val EM: 53.55% | Val F1: 71.47%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 71.47%)\n",
      "\n",
      "--- Epoch 8/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Summary:\n",
      "\tTrain Loss: 2.5427\n",
      "\tVal Loss  : 2.3255 | Val EM: 54.05% | Val F1: 72.32%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 72.32%)\n",
      "\n",
      "--- Epoch 9/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Summary:\n",
      "\tTrain Loss: 2.5027\n",
      "\tVal Loss  : 2.3352 | Val EM: 54.15% | Val F1: 72.68%\n",
      "\tNew best model saved to ./saved_models\\bidaf_bert_char_best.pt (F1: 72.68%)\n",
      "\n",
      "--- Epoch 10/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Summary:\n",
      "\tTrain Loss: 2.4699\n",
      "\tVal Loss  : 2.3566 | Val EM: 53.84% | Val F1: 72.23%\n",
      "\n",
      "Training complete.\n",
      "Best Validation F1: 72.68% (Model saved at ./saved_models\\bidaf_bert_char_best.pt)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T19:19:57.751548Z",
     "start_time": "2025-05-07T19:19:57.470639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "51368bead21a668a",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
